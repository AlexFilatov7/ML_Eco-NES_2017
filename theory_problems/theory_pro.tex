\documentclass[11pt, a4paper]{article}

% If you can't see cyrillic letters in R-studio choose
% File-Reopen with encoding
% utf8 is the preferred encoding


\input{title_bor_utf8_knitr}
\input{emetrix_preamble}




\usepackage[bibencoding = auto, backend = biber,
sorting = none]{biblatex}

\addbibresource{metrics_pro.bib}

\def \RR{\mathbb{R}}
\def \cN{\mathcal{N}}
\def \htheta{\hat{\theta}}

\title{Задачки для семинаров и домашки}
\author{Винни-Пух}
\date{\today}


% делаем короче интервал в списках
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}


\DeclareMathOperator{\Med}{Med}


\usepackage{answers} % дележка условий и ответов

%\newtheorem{problem}{Задача}
%\numberwithin{problem}{section}

\Newassociation{sol}{solution}{solution_file}
% sol — имя окружения внутри задач
% solution — имя окружения внутри solution_file
% solution_file — имя файла в который будет идти запись решений
% можно изменить далее по ходу
\Opensolutionfile{solution_file}[all_solutions]
% в квадратных скобках фактическое имя файла


% магия для автоматических гиперссылок задача-решение
\newlist{myenum}{enumerate}{3}
% \newcounter{problem}[chapter] % нумерация задач внутри глав
\newcounter{problem}[section]

\newenvironment{problem}%
{%
\refstepcounter{problem}%
%  hyperlink to solution
     \hypertarget{problem:{\thesection.\theproblem}}{} % нумерация внутри глав
     % \hypertarget{problem:{\theproblem}}{}
     \Writetofile{solution_file}{\protect\hypertarget{soln:\thesection.\theproblem}{}}
     %\Writetofile{solution_file}{\protect\hypertarget{soln:\theproblem}{}}
     \begin{myenum}[label=\bfseries\protect\hyperlink{soln:\thesection.\theproblem}{\thesection.\theproblem},ref=\thesection.\theproblem]
     % \begin{myenum}[label=\bfseries\protect\hyperlink{soln:\theproblem}{\theproblem},ref=\theproblem]
     \item%
    }%
    {%
    \end{myenum}}
% для гиперссылок обратно надо переопределять окружение
% это происходит непосредственно перед подключением файла с решениями





\begin{document}

% \maketitle % ставим сюда название, автора и время создания

\section{Линейная регрессия}

\begin{problem}
Рассмотрим задачу линейной регресии
\[
Q(w) = (y - Xw)^T(y - Xw) \to \min_{w}.
\]

\begin{enumerate}
\item Найдите $dQ(w)$ и $d^2Q(w)$.
\item Выведите формулу для оптимального $w$.
\item Выведите формулу для матрицы-шляпницы (hat-matrix), связывающей вектор фактических $y$ и вектор прогнозов $\hat y = H\cdot y$.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Рассмотрим задачу регрессии с одним признаком и без константы, $\hy_i = w \cdot x_i$. Решите в явном виде задачи МНК со штрафом:

\begin{enumerate}
\item $Q(w) = (y - \hy)^T (y - \hy) + \lambda w^2$;
\item $Q(w) = (y - \hy)^T (y - \hy) + \lambda |w|$;
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Храбрая и торопливая исследовательница Мишель хочет решить задачу линейной регрессии по $n$ наблюдениям с вектором $y$ и матрицей признаков $X$. Сначала исследовательница Мишель так торопилась, что совсем забыла последнее наблюдение и оценила задачу с более коротким вектором $y^{-}$ и матрицей $X^{-}$, где не хватает последней строки. Затем Мишель взяла правильную матрицу $X$, но неправильный вектор $y^*$, в котором она вместо фактического последнего наблюдения вектора $y$ вписала его прогноз, полученный с помощью регрессии с $y^{-1}$ и $X^{-}$.

\begin{enumerate}
\item Как связаны $\hat y_n^{-}$ и $\hat y_n^{*}$ (прогнозы для последнего наблюдения полученные по модели без последнего наблюдения и модели с неверным последним наблюдением)?
\item Как выглядит вектор, равный разнице $y - y^*$?
\item Какие величины находятся в векторе $H\cdot (y - y^*)$? Чему равна последняя, $n$-ая, компонента этого вектора? Выразите её через $H_{nn}$ и ошибку прогноза последнего наблюдения по модели без последнего наблюдения, $y_n - \hat y_n^{-}$.
\item Как связаны между собой ошибка прогноза $n$-го наблюдения по полной модели, ошибка прогноза $n$-го наблюдения по модели без последнего наблюдения и $H_{nn}$?
\item Как быстро провести кросс-валидацию с выкидыванием одного наблюдения для задачи линейной регрессии?
\end{enumerate}

\begin{sol}
\[
y_n - \hat y_n = (1 - H_{nn}) (y_n - \hat y_n^-)
\]
\end{sol}
\end{problem}


\section{Линейные классификаторы}

\begin{problem}
Рассмотрим плоскость в $\RR^3$, задаваемую уравнением $5x_1 + 6x_2 -7x_3 + 10 = 0$ и две точки, $A = (2, 1, 4)$ и $B = (4, 0, 4)$.
\begin{enumerate}
  \item Найдите любой вектор, перпендикулярный плоскости.
  \item Правда ли, что отрезок $AB$ пересекает плоскость?
  \item Найдите длину отрезка $AB$;
  \item Не находя расстояние от точек до плоскости, определите, во сколько раз точка $A$ дальше от плоскости, чем точка $B$;
  \item Найдите расстояние от точки $A$ до плоскости.
\end{enumerate}

\begin{sol}
$(5, 6, -7)$
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим простейший персептрон с константой, единственным входом $x_1$ и пороговой функцией активации. Подберите веса так, чтобы персептрон реализовывал логическое отрицание (в ответ на 0 выдавал 1, и наоборот).
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Рассмотрим простейший персептрон с константой, двумя входами $x_1$, $x_2$ и пороговой функцией активации.

\todo[inline]{Здесь ассистенты нарисуют в tikz картинку, достойную стоять вместо Джоконды в Лувре}

\begin{enumerate}
\item Подберите веса так, чтобы персептрон реализовывал логическое ИЛИ (OR).
\item Подберите веса так, чтобы персептрон реализовывал логическое И (AND).
\item Докажите, что веса невозможно подобрать так, чтобы персептрон реализовывал исключающее логическое ИЛИ (XOR).
\item Добавьте персептрону вход $x_3 = x_1 \cdot x_2$ и подберите веса так, чтобы персептрон реализовывал XOR.
\item Реализуйте XOR с помощью трёх персептронов с двумя входами и константой. Укажите веса и схему их взаимосвязей.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
В коробке завалялось три персептрона, у каждого два входа с константой и пороговая функция активации. Реализуйте с их помощью функцию
\[
y = \begin{cases}
1, \text{ если } x_2 \geq |x_1 - 3| + 2; \\
0, \text{ иначе}
\end{cases}.
\]
\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Рассмотрим следующий набор данных:

\begin{tabular}{ccc}
\toprule
$x_i$ & $z_i$ & $y_i$ \\
\midrule
-1 & -1 & 0 \\
1 & -1 & 0 \\
-1 & 1 & 0 \\
1 & 1 & 0 \\
0 & 2 & 1 \\
2 & 0 & 1 \\
0 & -2 & 1 \\
-2 & 0 & 1 \\
\bottomrule
\end{tabular}

\begin{enumerate}
\item Существует ли перспетрон с константой, двумя входами и пороговой функцией активации, способный идеально классифицировать $y_i$ на данной выборке? А хватит ли двух таких персептронов? А может хватит трёх?
\item Введите такое преобразование исходных признаков $h_i = h(x_i, z_i)$, при котором с идеальной классификацией $y_i$ справился бы даже персептрон с одним входом, константой и пороговой функцией активации.
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}





\begin{problem}
Бандерлог из Лога\footnote{деревня в Кадуйском районе Вологодской области} ведёт блог, любит считать логарифмы и оценивать логистические регрессии. С помощью нового алгоритма Бандерлог решил задачу классификации по трём наблюдениям и получил $b_i = \hat\P(y_i = 1|x_i)$.

\begin{tabular}{cc}
  \toprule
  $y_i$ & $b_i$ \\
  \midrule
  1 & 0.7 \\
  -1 & 0.2 \\
  -1 & 0.3 \\
  \bottomrule
\end{tabular}

\begin{enumerate}
\item Постройте ROC-кривую.
\item Найдите площадь под ROC-кривой и индекс Джини.
\item Постройте PR-кривую (кривая точность-полнота).
\item Найдите площадь под PR-кривой.
\item Как по-английски будет «бревно»?
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Классификатор Бандерлога имеет вид
\[
a_i = \begin{cases}
1, \text{ если } b_i > t; \\
-1, \text{ иначе.}
\end{cases}
\]

Докажите, что площадь под ROC-кривой равна вероятности того, случайно выбранный положительный объект окажется позже случайно выбранного отрицательного объекта, если объекты ранжированы по возрастанию величины $b_i$.
\begin{sol}
\end{sol}
\end{problem}






\begin{problem}
Все средние издалека выглядят одинаково, $\text{среднее}=f^{-1}(0.5f(x_1) + 0.5f(x_2))$.  Например, у среднего арифметического $f(t)=t$, у среднего гармонического $f(t)=1/t$.

\begin{enumerate}
  \item Какая $f$ используется для среднего геометрического?
\end{enumerate}

Для измерения качества бинарной классификации Ара использует среднее арифметическое точности и полноты, Гена — среднее геометрическое, а Гарик — среднее гармоническое.

\begin{enumerate}[resume]
  \item У кого будут выходить самые «качественные» и самые «некачественные» прогнозы?
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Бандерлог начинает все определения со слов «это доля правильных ответов»:
\begin{enumerate}
  \item accuracy — это доля правильных ответов\ldots
  \item точность (precision) — это доля правильных ответов\ldots
  \item полнота (recall) — это доля правильных ответов\ldots
  \item TPR — это доля правильных ответов\ldots
\end{enumerate}

Закончите определения Бандерлога так, чтобы они были, хм, правильными.
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Алгоритм бинарной классификации, придуманный Бандерлогом, выдаёт оценки вероятности $b_i = \hat\P(y_i=1 | x_i)$. Всего у Бандерлога 10000 наблюдений. Если ранжировать их по возрастанию $b_i$, то окажется что наблюдения с $y_i = 1$ занимают ровно места с  5501 по 5600.

Найдите площадь по ROC-кривой и площадь под PR-кривой.
\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Бандерлог собрал выборку из 900 муравьёв и 100 китов. Переменная $y_i$ равна $1$ для китов. Бандерлог хочет, чтобы его алгоритм классификации выдавал для каждого наблюдения число $b_i=f(x_i) \in [0;1]$, оценку вероятности того, что наблюдение является китом. В качестве признака Бандерлог использует количество глаз, не задумавшись о том, что оно равно двум и для муравьёв, и для китов.

Решите задачу минимизации эмпирической функции риска и найдите все $b_i$ для функций потерь:
\begin{enumerate}
  \item $L(y_i, b_i) = (y_i - b_i)^2$, если для муравьёв $y_i = 0$;
  \item $L(y_i, b_i) = |y_i - b_i|$, если для муравьёв $y_i = 0$;
  \item $L(y_i, b_i) = \begin{cases}
  -\log b_i, \text{ если } y_i = 1 \\
  -\log (1-b_i), \text{ иначе.}
  \end{cases}$;
  %\item $L(y_i, b_i) = \log (1 + \exp(-y \cdot b))$, если для муравьёв $y = -1$;
  \item $L(y_i, b_i) = \begin{cases}
  1/b_i, \text{ если } y_i = 1 \\
  1/(1-b_i), \text{ иначе.}
  \end{cases}$;
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Бандерлог утверждает, что открыл новую верхнюю границу для пороговой функции потерь, $\tilde{L}(M_i) = 1 + \frac{1}{\pi} \cdot \arctan(-x_i)$, где $M_i = y_i \cdot \langle w, x_i \rangle$. Прав ли бандерлог?
\begin{sol}
  Нет. Не выполнено $\tilde{L} \geq L$ для всех $M \in \RR$.
\end{sol}
\end{problem}


\begin{problem}
Бандерлог из Лога оценил логистическую регрессию по четырём наблюдениям и одному признаку с константой, получил $b_i = \hat\P(y_i = 1|x_i)$, но потерял последнее наблюдение:

\begin{tabular}{cc}
  \toprule
  $y_i$ & $b_i$ \\
  \midrule
  1 & 0.7 \\
  -1 & 0.2 \\
  -1 & 0.3 \\
  ? &  ? \\
  \bottomrule
\end{tabular}

\begin{enumerate}
\item Выпишите функцию потерь для задачи логистической регрессии.
\item Выпишите условие первого порядка по коэффициенту перед константой.
\item Помогите бандерлогу восстановить пропущенные значения!
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
У Бандерлога три наблюдения, первое наблюдение — кит, остальные — муравьи. Киты кодируются $y_i = 1$, муравьи — $y_i = -1$. На этот раз Бандерлог, чтобы быть уверенным, что $x_i$ различаются, сам лично определил $x_i = i$. После этого Бандерлог оценивает логистическую регрессию с константой.

\begin{enumerate}
  \item Выпишите эмпирическую функцию риска, которую минимизирует Бандерлог;
  \item При каких оценках коэффициентов логистической регрессии эта функция достигает своего минимума?
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Рассмотрим целевую функцию логистической регрессии с константой
\[
Q(w) = \frac{1}{\ell} \sum L(y_i, b_i),
\]
где $b_i = 1 / (1 + \exp( -\langle w, x_i\rangle)$ и $L(y_i, b_i) = \begin{cases}
-\log b_i, \text{ если } y_i = 1 \\
-\log (1-b_i), \text{ иначе.}
\end{cases}$.

\begin{enumerate}
\item Найдите $dQ(w)$ и $d^2Q(w)$;
\item Найдите $dQ(0)$ и $d^2Q(0)$;
\item Выпишите квадратичную аппроксимацию для $Q(w)$ в окрестности $w=0$;
\item С какой задачей совпадает задача минимизации квадратичной аппроксимации?
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Винни-Пух знает, что мёд бывает правильный, $honey_i=1$, и неправильный, $honey_i=0$. Пчёлы также бывают правильные, $bee_i=1$, и неправильные, $bee_i=0$. По 100 своим попыткам добыть мёд Винни-Пух составил таблицу сопряженности:

\begin{tabular}{c|cc}
\toprule
 & $honey_i=1$ & $honey_i=0$ \\
\midrule
$bee_i=1$ & 12 & 36 \\
$bee_i=0$ & 32 & 20 \\
\bottomrule
\end{tabular}

Винни-Пух использует логистическую регрессию с константой для прогнозирования правильности мёда с помощью правильности пчёл.

\begin{enumerate}
\item Какие оценки коэффициентов получит Винни-Пух?
\item Какой прогноз вероятности правильности мёда при встрече с неправильными пчёлами даёт логистическая модель? Как это число можно посчитать без рассчитывания коэффициентов?
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Винни-Пух оценил логистическую регрессию для прогнозирования правильности мёда от высоты дерева (м) $x_i$ и удалённости от дома (км) $z_i$: $\ln odds_i = 2+0.3x_i - 0.5z_i$.
\begin{enumerate}
\item Оцените вероятность того, что $y_i=1$ для $x=15$, $z=3.5$.
\item Оцените предельный эффект увеличения $x$ на единицу на вероятность того, что $y_i=1$ для $x=15$, $z=3.5$.
\item При каком значении $x$ предельный эффект увеличения $x$ на единицу в точке $z=3.5$ будет максимальным?
\end{enumerate}

\begin{sol}
Предельный эффект максимален он при максимальной производной $\Lambda'(\hat \beta_1 + \hat\beta_2x + \hat\beta_3z)$, то есть при $\hat \beta_1 + \hat\beta_2x + \hat\beta_3z=0$.
\end{sol}
\end{problem}

% \section{Хочу ещё задач!}



\section{Матрицы}


\begin{problem}
Известна матрица $X$,
\[
X = \begin{pmatrix}
1 & 1 \\
0 & 1 \\
-1 & 0 \\
\end{pmatrix};
\]

\begin{enumerate}
\item Найдите QR-разложение матрицы $X'X$;
\item Найдите QR-разложение матрицы $XX'$;
\item Найдите спектральное разложение матрицы $X'X$;
\item Найдите спектральное разложение матрицы $XX'$;
\item Найдите сингулярное разложение (SVD) матрицы $X$;
\end{enumerate}

\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
Объясните геометрический смысл QR, SVD и спектрального разложений.
\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Бандрелог выполнил SVD-разложение матрицы регрессоров $X$. Помогите Бандерлогу поскорее найти формулу для матрицы-шляпницы $H$, которая проецирует $y$ на пространство столбцов матрицы $X$, $\hat y = Hy$.
\begin{sol}
\end{sol}
\end{problem}




\begin{problem}
Бандрелог выполнил QR-разложение матрицы регрессоров $X$. Помогите Бандерлогу поскорее найти формулу для матрицы-шляпницы $H$, которая проецирует $y$ на пространство столбцов матрицы $X$, $\hat y = Hy$.
\begin{sol}
\end{sol}
\end{problem}

\section{Метод опорных векторов}



\begin{problem}
На плоскости имеются точки двух цветов. Красные: $(1,1)$, $(1,-1)$ и синие: $(-1,1)$, $(-1,-1)$.
\begin{enumerate}
\item Найдите разделяющую гиперплоскость методом опорных векторов при разных $C$.
\item Укажите опорные вектора.
\end{enumerate}



\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
На плоскости имеются точки двух цветов. Красные: $(1,1)$, $(1,-1)$ и синие: $(-1,1)$, $(-1,-1)$ и $(2,0)$.
\begin{enumerate}
\item Найдите разделяющую гиперплоскость методом опорных векторов при разных $C$.
\item Укажите опорные вектора.
\end{enumerate}



\begin{sol}
\end{sol}
\end{problem}




\begin{problem}
Эконометресса Авдотья решила использовать метод опорных векторов с гауссовским ядром с параметром $\sigma=1$ и штрафным коэффициентом $C=1$. Соответственно, она минимизировала целевую функцию

\[
\frac{w'w}{2} + C\sum_{i=1}^n \xi_i,
\]

где разделяющая плоскость задаётся $w'x-w_0=0$, а $\xi_i$ — размеры «заступа» за разделяющую полосу.

Затем Автдотья подумала, что неплохо бы выбрать наилучшие $C$ и $\sigma$. Ей лень было использовать кросс-валидацию, поэтому Авдотья минимизировала данную функцию по $C\geq 0$ и $\sigma\geq 0$. Какие значения она получила?


\begin{sol}
$C=0$ и $\sigma=+\infty$
\end{sol}
\end{problem}


\begin{problem}
Задан вектор $w=(2,3)$ и число $w_0=7$.

\begin{enumerate}
\item Нарисуйте прямые $\langle w, x\rangle=w_0$, $\langle w, x\rangle=w_0+1$, $\langle w, x\rangle=w_0-1$.
\item Найдите ширину полосы между $\langle w, x\rangle=w_0+1$ и $\langle w, x\rangle=w_0-1$.
\item Найдите расстояние от точки $(5,6)$ до прямой $\langle w, x\rangle=w_0-1$.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Заданы две прямые, $l_0$: $x^{(1)}+3x^{(2)} = 9$ и $l_1$: $x^{(1)}+3x^{(2)} = 13$. Найдите подходяющий вектор $w$ и число $w_0$ так, чтобы прямая $l_0$ записывалась как  $\langle w,x \rangle=w_0-1$, а прямая $l_1$ как  $\langle w,x\rangle=w_0 + 1$.
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Даны наблюдения

\begin{tabular}{ccc}
 $x^{(1)}$ & $x^{(2)}$ & $y$ \\
\midrule
 1 & 0 & 0 \\
 2 & 0 & 0 \\
 0 & 3 & 1 \\
 0 & 4 & 1 \\
\end{tabular}

\begin{enumerate}
\item Нарисуйте разделяющую полосу наибольшей ширины.
\item Решите задачу оптимизации
\[
\min_{w, w_0} \frac{1}{2} \langle w, w \rangle
\]

при ограничении: для $y_i=1$ выполнено условие $\langle w,x \rangle \geq w_0 + 1$, а для $y_i=0$ выполнено условие $\langle w,x \rangle \leq w_0 - 1$.
\item Для точки $x=(x^{(1)}, x^{(2)}) =(1, 1)$ найдите значение $\langle w,x \rangle-w_0$ и постройте прогноз $\hy$.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}

\begin{problem}

По картинке качественно решите задачу разделения точек:



\begin{minipage}{0.6\textwidth}
\begin{center}
%\begin{tikzpicture}[scale = 0.025]
\includegraphics[scale=0.2]{armada.png}
%\input{armada.tikz}
%\end{tikzpicture}
\end{center}
\end{minipage}


Целевая функция имеет вид:
\[
\min_{w, w_0} \frac{1}{2} w'w + C \sum_{i=1}^n \xi_i
\]

Уравнение разделяющей поверхности — $w'x = w_0$, уравнения краёв полосы: $w'x=w_0+1$ и $w'x=w_0-1$. Нарушителями считаются наблюдения, которые попали на нейтральную полосу или на чужую территорию. Здесь $\xi_i = |w| \cdot d_i$, где $d_i$ — длина «заступ» наблюдения за черту «своих».



\begin{enumerate}
\item Как пройдёт разделяющая полоса при $C=1$? Найдите $w$, $w_0$, и величины штрафов $\xi_i$.
\item Как пройдёт разделяющая полоса при $C=+\infty$? Найдите $w$, $w_0$, и величины штрафов $\xi_i$.
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}




\begin{problem}
ююю
\begin{sol}
\end{sol}
\end{problem}


\newpage
\section{Ядра к бою!}


\begin{problem}
Ядерная функция, скалярное произведение в расширяющем пространстве, имеет вид $K(a,b)=\exp(-|a-b|^2)$.

Имеются вектора $a=(1,1,1)$ и $b=(1,2,0)$.

Найдите длину векторов и косинус угла между ними в исходном и расширяющем пространстве.


\begin{sol}
В исходном пространстве: $|\vec{a}|=\sqrt{3}$, $|\vec{b}|=\sqrt{5}$, $\cos(\vec{a},\vec{b})=\sqrt{0.6}$.

В расширяющем пространстве: $|h(\vec{a})|=1$, $|h(\vec{b})|=1$, $\cos(h(\vec{a}),h(\vec{b}))=e^{-2}$.
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим два вектора, $v_1=(1, 1, 2)$ и $v_2=(1, 1, 1)$. Переход в спрямляющее пространство осуществляется с помощью гауссовской ядерной функции с параметром $\gamma$, $k(v,v')=\exp(-\gamma |v-v'|^2)$.

\begin{enumerate}
\item  Как от $\gamma$ зависят длины векторов в спрямляющем пространстве?
\item  Как от $\gamma$ зависит угол между векторами в спрямляющем пространстве?
\end{enumerate}



\begin{sol}
Длина равна 1 и не зависит от $\gamma$. При $\gamma \approx 0$ вектора примерно совпадают, при больших $\gamma$ вектора примерно ортогональны.
\end{sol}
\end{problem}





\begin{problem}
Имеются три наблюдения $A$, $B$ и $C$:

\begin{tabular}{ccc}
 & $x$ & $y$ \\
\midrule
$A$ & 1 & -2 \\
$B$ & 2 & 1 \\
$C$ & 3 & 0 \\
\end{tabular}

\begin{enumerate}
\item Найдите расстояние $AB$ и косинус угла $ABC$.
\item Найдите расстояние $AB$ и косинус угла $ABC$ в расширенном пространстве с помощью гауссовского ядра с $K(x,x') =\exp(-|x-x'|^2)$.
\item Найдите расстояние $AB$ и косинус угла $ABC$ в расширенном пространстве с помощью полиномиального ядра второй степени.
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Переход из двумерного пространства в расширяющее задан функцией
\[
f : (x_1,x_2) \to (1,x_1,x_2,3x_1 x_2, 2x_1^2, 4x_2^2).
\]
Найдите соответствующую ядерную функцию.


\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Ядерная функция имеет вид
\[
K(x,y)=x_1^2y_1^2+x_2^2y_2^2+2x_1x_2y_1y_2.
\]
Как может выглядеть функция $f:\R^2\to\R^3$ переводящие исходные векторы в расширенное пространство?


\begin{sol}
$f(x_1,x_2)=(x_1^2,x_2^2,\sqrt{2}x_1x_2)$
\end{sol}
\end{problem}



\section{Двойственные задачи}


\begin{problem}
Выпишите двойственную задачу для минимизации $x_1^2 + x_2^2 + x_3^2$ при ограничении $2x_1 + 3x_2 +5x_3 = 10$.
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Выпишите двойственную задачу для $x_1 + 2x_2 + 3x_3 \to \max$ при ограничениях $x_1 + x_2 + x_3 \leq 10$, $2x_1+x_2+x_3 \leq 10$, все $x_i \geq 0$.
\begin{sol}
\end{sol}
\end{problem}





\begin{problem}
Выпишите двойственную задачу для максимизации $1/x_1 + 2/x_2$ при ограничении $2x_1 + 3x_2 = 10$ и $x_1 \in [1;10]$, $x_2 \in [2;6]$.
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Выпишите двойственную задачу для минимизации $f(x) = \frac{1}{2} x'Hx + g'x$ при ограничении $A'x=b$.
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Выпишите двойственную задачу для минимизации $f(x) = \frac{1}{2} x'Hx + g'x$ при ограничении $A'x \leq b$.
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Выпишите прямую и двойственную задачу для метода опорных векторов в исходном пространстве.
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Выпишите прямую и двойственную задачу для метода опорных векторов в спрямляющем пространстве с использованием ядра $K(.,.)$.
\begin{sol}
\end{sol}
\end{problem}





\section{Метод главных компонент}



\begin{problem}
Найдите прямую, у которой сумма квадратов расстояний до точек $(0,0)$, $(1, 1)$, $(2, 1)$ будет минимальной. Чему равна при этом доля объяснённого разброса точек?
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Есть две переменных, $x = (1, 0, 0, 3)'$, $z = (3, 2, 0, 3)'$. Найдите первую и вторую главные компоненты. 
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Известна матрица выборочных ковариаций трёх переменных. Для удобства будем считать, что переменные уже центрированы.

\[
\begin{pmatrix}
4 & 1 & -1 \\
1 & 5 & 0 \\
-1 & 0 & 9
\end{pmatrix}
\]

\begin{enumerate}
\item Выразите первую и вторую главные компоненты через три исходных переменных.
\item Выразите первую и вторую главные компоненты, через три исходных переменных, если перед методом главных компонент переменные необходимо стандартизировать.
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}




\begin{problem}
Пионеры, Крокодил Гена и Чебурашка собирали металлолом несколько дней подряд. В распоряжение иностранной шпионки, гражданки Шапокляк, попали ежедневные данные по количеству собранного металлолома: вектор $g$ — для Крокодила Гены, вектор $h$ — для Чебурашки и вектор $x$ — для Пионеров. Гена и Чебурашка собирали вместе, поэтому выборочная корреляция $\sCorr(g,h)=-0.9$. Гена и Чебурашка собирали независимо от Пионеров, поэтому выборочные корреляции $\sCorr(g,x)=0$, $\sCorr(h,x)=0$. Если регрессоры $g$, $h$ и $x$ центрировать и нормировать, то получится матрица $\tilde{X}$.
\begin{enumerate}
\item Найдите параметр обусловленности матрицы $(\tilde{X}'\tilde{X})$.
\item Вычислите одну или две главные компоненты (выразите их через вектор-столбцы матрицы. $\tilde{X}$), объясняющие не менее 70\% общей выборочной дисперсии регрессоров.
\item Шпионка Шапокляк пытается смоделировать ежедневный выпуск танков, $y$. Выразите оценки коэффициентов регрессии $y = \beta_1 + \beta_2 g +\beta_3 h +\beta_4 x+\varepsilon$ через оценки коэффициентов регрессии на главные компоненты, объясняющие не менее 70\% общей выборочной дисперсии.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}



\Closesolutionfile{solution_file}


% для гиперссылок на условия
% http://tex.stackexchange.com/questions/45415
\renewenvironment{solution}[1]{%
         % add some glue
         \vskip .5cm plus 2cm minus 0.1cm%
         {\bfseries \hyperlink{problem:#1}{#1.}}%
}%
{%
}%

%\section{Решения}
%\input{all_solutions}

%\section{Источники мудрости}
%\printbibliography[heading=none]


\end{document}
